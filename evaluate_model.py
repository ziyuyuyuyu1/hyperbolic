#!/usr/bin/env python3
"""
Script to evaluate a model on test data generated by generate_test_data.py.
Provides comprehensive metrics including accuracy, loss, perplexity, and operator-specific analysis.
"""

import json
import torch
import argparse
import sys
import os
from typing import List, Dict, Any, Tuple
from collections import defaultdict
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM

# Add src to sys.path so we can import the custom model registration
sys.path.append(os.path.join(os.path.dirname(__file__), "src"))
from llamafactory.model import hyperbolic_utils  # This runs the registration code


class ModelEvaluator:
    """Evaluate a model on mathematical calculation test data."""
    
    def __init__(self, model_path: str, device: str = "auto"):
        """Initialize the evaluator with model and tokenizer."""
        self.model_path = model_path
        
        # Set device
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        print(f"Loading model from: {model_path}")
        print(f"Using device: {self.device}")
        
        # Load tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        num_added_tokens = self.tokenizer.add_tokens(new_tokens=["<|eom_id|>"], special_tokens=True)
        print(f"Added {num_added_tokens} new tokens")
        
        self.model = AutoModelForCausalLM.from_pretrained(model_path)
        self.model.to(self.device)
        self.model.eval()
        
        print("Model loaded successfully!")
    
    def extract_predicted_answer(self, output_text: str) -> str:
        """Extract the predicted answer from model output."""
        # Try to extract answer after <|eom_id|> token
        if '<|eom_id|>' in output_text:
            parts = output_text.split('<|eom_id|>')
            if len(parts) > 1:
                # Take everything after <|eom_id|> and clean it up
                answer_part = parts[1].strip()
                # Remove any remaining special tokens and whitespace
                answer_part = answer_part.replace('<unk>', '').replace('<bos>', '').replace('<eos>', '').strip()
                return answer_part
        
        # Fallback: try to extract after '='
        if '=' in output_text:
            return output_text.split('=')[-1].strip()
        
        # If no clear pattern, return the last part of the output
        return output_text.strip()
    
    def evaluate_single_sample(self, input_text: str, expected_answer: str, 
                             max_new_tokens: int = 5, do_sample: bool = False) -> Dict[str, Any]:
        """Evaluate a single test sample."""
        # Tokenize input
        inputs = self.tokenizer(input_text, return_tensors="pt")
        input_ids = inputs["input_ids"].to(self.device)
        
        # Generate output
        with torch.no_grad():
            output_ids = self.model.generate(
                input_ids,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # Decode output
        output_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=False)
        predicted_answer = self.extract_predicted_answer(output_text)
        
        # Calculate loss for the expected answer
        target_text = input_text + expected_answer
        target_inputs = self.tokenizer(target_text, return_tensors="pt")
        target_ids = target_inputs["input_ids"].to(self.device)
        
        with torch.no_grad():
            outputs = self.model(target_ids, labels=target_ids)
            loss = outputs.loss.item()
        
        # Calculate metrics
        is_correct = predicted_answer.strip() == expected_answer.strip()
        perplexity = torch.exp(torch.tensor(loss)).item()
        
        return {
            "input": input_text,
            "expected_answer": expected_answer,
            "predicted_answer": predicted_answer,
            "output_text": output_text,
            "is_correct": is_correct,
            "loss": loss,
            "perplexity": perplexity
        }
    
    def evaluate_dataset(self, test_data: List[Dict[str, str]], 
                        max_new_tokens: int = 5, do_sample: bool = False) -> Dict[str, Any]:
        """Evaluate the model on the entire dataset."""
        results = []
        total_correct = 0
        
        print(f"Evaluating {len(test_data)} samples...")
        
        for i, sample in enumerate(test_data):
            if (i + 1) % 100 == 0:
                print(f"Processed {i + 1}/{len(test_data)} samples...")
            
            result = self.evaluate_single_sample(
                sample["input"], 
                sample["expected_answer"],
                max_new_tokens=max_new_tokens,
                do_sample=do_sample
            )
            
            # Add original sample info
            result["expression"] = sample.get("expression", "")
            result["operator"] = sample.get("operator", "")
            
            results.append(result)
            
            if result["is_correct"]:
                total_correct += 1
        
        # Calculate overall metrics
        accuracy = total_correct / len(test_data)
        avg_loss = np.mean([r["loss"] for r in results])
        avg_perplexity = np.mean([r["perplexity"] for r in results])
        
        # Calculate operator-specific metrics
        operator_metrics = defaultdict(lambda: {"correct": 0, "total": 0, "losses": [], "perplexities": []})
        
        for result in results:
            op = result.get("operator", "unknown")
            operator_metrics[op]["total"] += 1
            operator_metrics[op]["losses"].append(result["loss"])
            operator_metrics[op]["perplexities"].append(result["perplexity"])
            
            if result["is_correct"]:
                operator_metrics[op]["correct"] += 1
        
        # Calculate operator-specific accuracies
        for op in operator_metrics:
            operator_metrics[op]["accuracy"] = operator_metrics[op]["correct"] / operator_metrics[op]["total"]
            operator_metrics[op]["avg_loss"] = np.mean(operator_metrics[op]["losses"])
            operator_metrics[op]["avg_perplexity"] = np.mean(operator_metrics[op]["perplexities"])
        
        return {
            "overall": {
                "accuracy": accuracy,
                "total_correct": total_correct,
                "total_samples": len(test_data),
                "avg_loss": avg_loss,
                "avg_perplexity": avg_perplexity
            },
            "operator_metrics": dict(operator_metrics),
            "detailed_results": results
        }
    
    def print_results(self, results: Dict[str, Any], show_examples: int = 5):
        """Print evaluation results in a formatted way."""
        overall = results["overall"]
        operator_metrics = results["operator_metrics"]
        
        print("\n" + "="*60)
        print("EVALUATION RESULTS")
        print("="*60)
        
        print(f"\nOverall Performance:")
        print(f"  Accuracy: {overall['accuracy']:.4f} ({overall['total_correct']}/{overall['total_samples']})")
        print(f"  Average Loss: {overall['avg_loss']:.4f}")
        print(f"  Average Perplexity: {overall['avg_perplexity']:.4f}")
        
        print(f"\n" + "="*40)
        print("OPERATOR-SPECIFIC PERFORMANCE")
        print("="*40)
        
        # Sort operators by accuracy for better visualization
        sorted_operators = sorted(operator_metrics.items(), key=lambda x: x[1]['accuracy'], reverse=True)
        
        print(f"{'Operator':<8} {'Accuracy':<12} {'Correct/Total':<15} {'Loss':<10} {'Perplexity':<12}")
        print("-" * 60)
        
        for op, metrics in sorted_operators:
            print(f"{op:<8} {metrics['accuracy']:<12.4f} {metrics['correct']:<7}/{metrics['total']:<7} "
                  f"{metrics['avg_loss']:<10.4f} {metrics['avg_perplexity']:<12.4f}")
        
        # Show operator ranking
        print(f"\nOperator Ranking (by accuracy):")
        for i, (op, metrics) in enumerate(sorted_operators, 1):
            print(f"  {i}. {op}: {metrics['accuracy']:.4f} ({metrics['correct']}/{metrics['total']})")
        
        # Show detailed breakdown
        print(f"\nDetailed Operator Breakdown:")
        for op, metrics in sorted_operators:
            print(f"\n{op} Operations:")
            print(f"  • Accuracy: {metrics['accuracy']:.4f} ({metrics['correct']}/{metrics['total']})")
            print(f"  • Average Loss: {metrics['avg_loss']:.4f}")
            print(f"  • Average Perplexity: {metrics['avg_perplexity']:.4f}")
            print(f"  • Success Rate: {metrics['correct']/metrics['total']*100:.1f}%")
        
        # Show example results
        if show_examples > 0:
            print(f"\n" + "="*40)
            print("EXAMPLE RESULTS")
            print("="*40)
            
            print(f"\nFirst {show_examples} samples:")
            for i, result in enumerate(results["detailed_results"][:show_examples]):
                status = "✓" if result["is_correct"] else "✗"
                op = result.get("operator", "?")
                print(f"\n{i+1}. {status} [{op}] Expression: {result['expression']}")
                print(f"   Expected: {result['expected_answer']}")
                print(f"   Predicted: {result['predicted_answer']}")
                print(f"   Loss: {result['loss']:.4f}")
        
        # Show some incorrect examples by operator
        incorrect_results = [r for r in results["detailed_results"] if not r["is_correct"]]
        if incorrect_results and show_examples > 0:
            print(f"\n" + "="*40)
            print("INCORRECT EXAMPLES BY OPERATOR")
            print("="*40)
            
            # Group incorrect examples by operator
            incorrect_by_operator = defaultdict(list)
            for result in incorrect_results:
                op = result.get("operator", "unknown")
                incorrect_by_operator[op].append(result)
            
            for op in sorted(incorrect_by_operator.keys()):
                op_incorrect = incorrect_by_operator[op]
                print(f"\n{op} Operations (Incorrect Examples):")
                for i, result in enumerate(op_incorrect[:min(show_examples, len(op_incorrect))]):
                    print(f"  {i+1}. Expression: {result['expression']}")
                    print(f"     Expected: {result['expected_answer']}")
                    print(f"     Predicted: {result['predicted_answer']}")
                    print(f"     Full Output: {result['output_text']}")
    
    def save_operator_results(self, results: Dict[str, Any], output_dir: str):
        """Save operator-specific results to separate files."""
        os.makedirs(output_dir, exist_ok=True)
        
        operator_metrics = results["operator_metrics"]
        detailed_results = results.get("detailed_results", [])
        
        # Save overall operator summary
        operator_summary = {
            "overall": results["overall"],
            "operator_metrics": operator_metrics,
            "operator_ranking": sorted(operator_metrics.items(), key=lambda x: x[1]['accuracy'], reverse=True)
        }
        
        summary_file = os.path.join(output_dir, "operator_summary.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(operator_summary, f, ensure_ascii=False, indent=2)
        
        print(f"Operator summary saved to: {summary_file}")
        
        # Save detailed results for each operator
        for op, metrics in operator_metrics.items():
            op_results = [r for r in detailed_results if r.get("operator") == op]
            
            op_file = os.path.join(output_dir, f"{op}_operations.json")
            with open(op_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "operator": op,
                    "metrics": metrics,
                    "results": op_results
                }, f, ensure_ascii=False, indent=2)
            
            print(f"{op} operations ({len(op_results)} samples) saved to: {op_file}")
        
        # Save incorrect examples by operator
        incorrect_results = [r for r in detailed_results if not r["is_correct"]]
        if incorrect_results:
            incorrect_by_operator = defaultdict(list)
            for result in incorrect_results:
                op = result.get("operator", "unknown")
                incorrect_by_operator[op].append(result)
            
            incorrect_file = os.path.join(output_dir, "incorrect_examples_by_operator.json")
            with open(incorrect_file, 'w', encoding='utf-8') as f:
                json.dump(dict(incorrect_by_operator), f, ensure_ascii=False, indent=2)
            
            print(f"Incorrect examples by operator saved to: {incorrect_file}")

    def save_results(self, results: Dict[str, Any], output_file: str):
        """Save evaluation results to a JSON file."""
        os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else ".", exist_ok=True)
        
        # Convert numpy types to native Python types for JSON serialization
        def convert_numpy(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            return obj
        
        # Recursively convert numpy types
        def recursive_convert(obj):
            if isinstance(obj, dict):
                return {k: recursive_convert(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [recursive_convert(v) for v in obj]
            else:
                return convert_numpy(obj)
        
        results_serializable = recursive_convert(results)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results_serializable, f, ensure_ascii=False, indent=2)
        
        print(f"\nResults saved to: {output_file}")


def load_test_data(data_file: str) -> List[Dict[str, str]]:
    """Load test data from file."""
    with open(data_file, 'r', encoding='utf-8') as f:
        if data_file.endswith('.jsonl'):
            data = [json.loads(line) for line in f]
        else:
            data = json.load(f)
    return data


def main():
    parser = argparse.ArgumentParser(description="Evaluate a model on mathematical calculation test data")
    parser.add_argument("--model_path", "-m", type=str, required=True,
                       help="Path to the model to evaluate")
    parser.add_argument("--test_data", "-t", type=str, required=True,
                       help="Path to test data file (JSON or JSONL)")
    parser.add_argument("--output", "-o", type=str, default="evaluation_results.json",
                       help="Output file for results (default: evaluation_results.json)")
    parser.add_argument("--output_dir", type=str, default=None,
                       help="Output directory (if specified, output file will be placed in this directory)")
    parser.add_argument("--device", "-d", type=str, default="auto",
                       help="Device to use (auto, cuda, cpu) (default: auto)")
    parser.add_argument("--max_new_tokens", type=int, default=5,
                       help="Maximum new tokens to generate (default: 5)")
    parser.add_argument("--do_sample", action="store_true",
                       help="Use sampling instead of greedy decoding")
    parser.add_argument("--show_examples", type=int, default=5,
                       help="Number of example results to show (default: 5)")
    parser.add_argument("--save_detailed", action="store_true",
                       help="Save detailed results for each sample")
    parser.add_argument("--save_operator_results", action="store_true",
                       help="Save operator-specific results to separate files")
    
    args = parser.parse_args()
    
    # Handle output directory
    if args.output_dir:
        os.makedirs(args.output_dir, exist_ok=True)
        # Extract filename from output path
        filename = os.path.basename(args.output)
        output_path = os.path.join(args.output_dir, filename)
    else:
        output_path = args.output
    
    # Load test data
    print(f"Loading test data from: {args.test_data}")
    test_data = load_test_data(args.test_data)
    print(f"Loaded {len(test_data)} test samples")
    
    # Initialize evaluator
    evaluator = ModelEvaluator(args.model_path, args.device)
    
    # Run evaluation
    print(f"\nStarting evaluation...")
    results = evaluator.evaluate_dataset(
        test_data,
        max_new_tokens=args.max_new_tokens,
        do_sample=args.do_sample
    )
    
    # Print results
    evaluator.print_results(results, show_examples=args.show_examples)
    
    # Save results
    if not args.save_detailed:
        # Remove detailed results to save space
        results.pop("detailed_results", None)
    
    # Save operator-specific results if requested
    if args.save_operator_results and args.output_dir:
        evaluator.save_operator_results(results, args.output_dir)
    
    evaluator.save_results(results, output_path)


if __name__ == "__main__":
    main() 